{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging face pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c34773388354839bc394bd6fc53a527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pkalanski\\Anaconda3\\envs\\llm-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Pkalanski\\.cache\\huggingface\\hub\\models--rakeshkiriyath--gpt2Medium_text_to_sql. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f628a6d40c3a407bb575951fe373a7d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a42e5ddb314497dab3929f3d1588d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40780f3f567441c5b9500b9feb258ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/525 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e588049e7fe4a7e97f551c424f1fef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/999k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8812d15726842d2b51650382c1f4ca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32bbb4e19c9f4b08ada5f79aa6abd23e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/583 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=\"rakeshkiriyath/gpt2Medium_text_to_sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'I need a list of employees who joined in the company last 6 months with a salary hike of 30%SELECT employees FROM employees WHERE joined_in_company = \"6 months ago\" AND salary_hike = \"30%\"'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe('I need a list of employees who joined in the company last 6 months with a salary hike of 30%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Give me loginid,status,company of a user who is mapped to the organization XYZ:XYZSELECT login_id,status,company FROM Accounts AS T1 JOIN Organizations as O ON T1.id = T2.organ'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe('Give me loginid,status,company of a user who is mapped to the organization XYZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLama inde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.llamaindex.ai/en/stable/api_reference/llms/huggingface/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pkalanski\\Anaconda3\\envs\\llm-env\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:160: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.huggingface import HuggingFaceLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"rakeshkiriyath/gpt2Medium_text_to_sql\"\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name=model_id,\n",
    "    tokenizer_name=model_id,\n",
    "    # context_window=3900,\n",
    "    # max_new_tokens=256,\n",
    "    # model_kwargs={\"quantization_config\": quantization_config},\n",
    "    # generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
    "    # messages_to_prompt=messages_to_prompt,\n",
    "    # completion_to_prompt=completion_to_prompt,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text='SELECT loginid,status,company FROM user_mapped_to_organization WHERE organization = \"XYZ\"', additional_kwargs={}, raw={'model_output': tensor([[23318,   502, 17594,   312,    11, 13376,    11, 39722,   286,   257,\n",
       "          2836,   508,   318, 27661,   284,   262,  4009, 41420,    57, 46506,\n",
       "         17594,   312,    11, 13376,    11, 39722, 16034,  2836,    62,    76,\n",
       "          6320,    62,  1462,    62,  9971,  1634, 33411,  4009,   796,   366,\n",
       "         34278,    57,     1, 50256]])}, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.complete('Give me loginid,status,company of a user who is mapped to the organization XYZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SELECT \n",
      "SELECT \n",
      "SELECT \n",
      "SELECT \n",
      "SELECT \n",
      "SELECT \n",
      "SELECT loginid,status,company \n",
      "SELECT loginid,status,company FROM \n",
      "SELECT loginid,status,company FROM \n",
      "SELECT loginid,status,company FROM \n",
      "SELECT loginid,status,company FROM \n",
      "SELECT loginid,status,company FROM \n",
      "SELECT loginid,status,company FROM \n",
      "SELECT loginid,status,company FROM \n",
      "SELECT loginid,status,company FROM \n",
      "SELECT loginid,status,company FROM \n",
      "SELECT loginid,status,company FROM user_mapped_to_organization \n",
      "SELECT loginid,status,company FROM user_mapped_to_organization WHERE \n",
      "SELECT loginid,status,company FROM user_mapped_to_organization WHERE organization \n",
      "SELECT loginid,status,company FROM user_mapped_to_organization WHERE organization = \n",
      "SELECT loginid,status,company FROM user_mapped_to_organization WHERE organization = \n",
      "SELECT loginid,status,company FROM user_mapped_to_organization WHERE organization = \n",
      "SELECT loginid,status,company FROM user_mapped_to_organization WHERE organization = \n",
      "SELECT loginid,status,company FROM user_mapped_to_organization WHERE organization = \n",
      "SELECT loginid,status,company FROM user_mapped_to_organization WHERE organization = \"XYZ\"\n"
     ]
    }
   ],
   "source": [
    "for o in llm.stream_complete('Give me loginid,status,company of a user who is mapped to the organization XYZ'):\n",
    "    print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text='SELECT order_count_lifetime_orders FROM table_name_95 WHERE customer_id = \"redshift\"', additional_kwargs={}, raw={'model_output': tensor([[  198,  1639,   389,   257,  2297, 30846,  5887,    13,   198,   198,\n",
       "          5492,  1037,   284,  7716,   257,  2297, 30846, 12405,   284,  3280,\n",
       "           262,  1808,    13,  3406,  2882,   815, 22224,   307,  1912,   319,\n",
       "           262,  1813,  4732,   290,  1061,   262,  2882,  9949,   290,  5794,\n",
       "          7729,    13,   198,   198, 18604,    51,  2977,   198,    12,  1388,\n",
       "            13, 23144,   364,     7, 23144,   263,    62,   312,    11, 23144,\n",
       "           263,    62,  3672,    11,  9127,    62, 36195,  8079,    62,  6361,\n",
       "            11, 11085,    62, 24071,    62,   265,    11, 12957,    62, 24071,\n",
       "            62,   265,    11, 36195,  8079,    62,  2777,   437,    62,  5310,\n",
       "           897,    11, 36195,  8079,    62, 19290,    62, 20333,    11, 36195,\n",
       "          8079,    62,  2777,   437,    11, 23144,   263,    62,  4906,     8,\n",
       "           198,    12,  1388,    13,  6361,     7,  2875,    62,   312,    11,\n",
       "         23144,   263,    62,   312,    11,  2875,    62, 23350,    11, 24071,\n",
       "            62,   265,    11,  2875,    62, 15805,    11,   271,    62, 19425,\n",
       "            62,  2875,    11,   271,    62,  7109,   676,    62,  2875,     8,\n",
       "           198,   198, 18604, 31077, 31072,   198,    16,    13,  1002,   262,\n",
       "          2810,  4732,   318,  6751,    11,  3387,  7716,   257,  4938, 12405,\n",
       "          1231,   597, 18681,   329,   262,  1808,    13,   198,    17,    13,\n",
       "          1002,   262,  2810,  4732,   318, 19022,    11,  3387,  4727,  1521,\n",
       "           340,   460,   470,   307,  7560,    13,   198,    18,    13,  4222,\n",
       "           779,   262,   749,  5981,  3084,     7,    82,   737,   198,    20,\n",
       "            13,  4222,  5794,   262, 12405,   878, 14409,    13,   198,    21,\n",
       "            13,  4222,  1464,  3031,   351,   257,  4938,   880,    12, 12214,\n",
       "         19449,  2134,   351,   262,  1708,  5794,   198,   198, 18604, 31077,\n",
       "         18980,   198,    90,   198,   220,   220,   220,   366, 22766,  1298,\n",
       "           366,    32,  7560, 16363, 12405,   618,  4732,   318,  6751, 33283,\n",
       "           198,   220,   220,   220,   366,  1069, 11578,   341,  1298,   366,\n",
       "          2025,  7468,   286,  9894,   284,  7716,   262, 12405,   526,   198,\n",
       "            92,   198,   198, 18604, 24361,   198,  2875,   954,   416,  6491,\n",
       "           198, 46506,  1502,    62,  9127,    62, 36195,  8079,    62,  6361,\n",
       "         16034,  3084,    62,  3672,    62,  3865, 33411,  6491,    62,   312,\n",
       "           796,   366,   445, 30846,     1, 50256]])}, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "You are a Redshift expert.\n",
    "\n",
    "Please help to generate a Redshift query to answer the question. Your response should ONLY be based on the given context and follow the response guidelines and format instructions.\n",
    "\n",
    "===Tables\n",
    "- main.customers(customer_id,customer_name,count_lifetime_orders,first_ordered_at,last_ordered_at,lifetime_spend_pretax,lifetime_tax_paid,lifetime_spend,customer_type)\n",
    "- main.orders(order_id,customer_id,order_total,ordered_at,order_cost,is_food_order,is_drink_order)\n",
    "\n",
    "===Response Guidelines\n",
    "1. If the provided context is sufficient, please generate a valid query without any explanations for the question.\n",
    "2. If the provided context is insufficient, please explain why it can't be generated.\n",
    "3. Please use the most relevant table(s).\n",
    "5. Please format the query before responding.\n",
    "6. Please always respond with a valid well-formed JSON object with the following format\n",
    "\n",
    "===Response Format\n",
    "{\n",
    "    \"query\": \"A generated SQL query when context is sufficient.\",\n",
    "    \"explanation\": \"An explanation of failing to generate the query.\"\n",
    "}\n",
    "\n",
    "===Question\n",
    "order count by customer\n",
    "\"\"\"\n",
    "llm.complete(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccef4a5a22dd229f1c3745087ddeb5ed84244d50210325929ab29d3e19e4eb9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
